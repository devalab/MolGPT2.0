{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from build_corpus import build_corpus\n",
    "from build_vocab import WordVocab\n",
    "from utils import split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "import wandb\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/DOCKSTRING/target_specific/LCK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# df['qeds'] = scaler.fit_transform(df['qeds'].values.reshape(-1,1))\n",
    "# df['tpsas'] = scaler.fit_transform(df['tpsas'].values.reshape(-1,1))\n",
    "# df['logps'] = scaler.fit_transform(df['logps'].values.reshape(-1,1))\n",
    "# df['affinity'] = scaler.fit_transform(df['affinity'].values.reshape(-1,1))\n",
    "\n",
    "affinity_scaler = MinMaxScaler()\n",
    "qed_scaler = MinMaxScaler()\n",
    "logp_scaler = MinMaxScaler()\n",
    "tpsas_scaler = MinMaxScaler()\n",
    "\n",
    "affinity_scaler.fit(df['affinity'].values.reshape(-1,1))\n",
    "qed_scaler.fit(df['qeds'].values.reshape(-1,1))\n",
    "logp_scaler.fit(df['logps'].values.reshape(-1,1))\n",
    "tpsas_scaler.fit(df['tpsas'].values.reshape(-1,1))\n",
    "\n",
    "df['qeds'] = qed_scaler.transform(df['qeds'].values.reshape(-1,1))\n",
    "df['logps'] = logp_scaler.transform(df['logps'].values.reshape(-1,1))\n",
    "df['tpsas'] = tpsas_scaler.transform(df['tpsas'].values.reshape(-1,1))\n",
    "df['affinity'] = affinity_scaler.transform(df['affinity'].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMI_MAX_SIZE  241\n"
     ]
    }
   ],
   "source": [
    "SMI_MAX_SIZE= -1\n",
    "with open('../data/DOCKSTRING/smiles_corpus.txt', 'w') as f:\n",
    "    train = []\n",
    "    test = []        \n",
    "    for i, row in df.iterrows():\n",
    "        if row['split'] == \"test\":\n",
    "            test.append(list(row.values))\n",
    "        else:\n",
    "            train.append(list(row.values))\n",
    "        f.write(split(row['smiles'] +'\\n'))\n",
    "        \n",
    "        if SMI_MAX_SIZE < len(row['smiles']):\n",
    "            SMI_MAX_SIZE = len(row['smiles'])\n",
    "f.close()\n",
    "print(\"SMI_MAX_SIZE \", SMI_MAX_SIZE)\n",
    "train_df = pd.DataFrame(train, columns=df.columns)\n",
    "test_df = pd.DataFrame(test, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n"
     ]
    }
   ],
   "source": [
    "SMI_MAX_SIZE = 300\n",
    "SMI_MIN_FREQ=1\n",
    "with open(\"../data/DOCKSTRING/smiles_corpus.txt\", \"r\") as f:\n",
    "    smiles_vocab = WordVocab(f, max_size=SMI_MAX_SIZE, min_freq=SMI_MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTargetDataset(Dataset):\n",
    "    def __init__(self, dataframe, SmilesVocab, properties_list):\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_vocab = SmilesVocab\n",
    "        self.property_list = properties_list\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        smiles, properties, affinities= [],[],[]\n",
    "        smiles_encoding = []\n",
    "        for i, row in self.dataframe.iterrows():\n",
    "            smi = row['smiles']\n",
    "            # newsmi = Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "            newsmi = smi\n",
    "            smiles.append(newsmi)\n",
    "            smiles_encoding.append(self.smiles_vocab.to_seq(split(newsmi), seq_len=SMI_MAX_SIZE))\n",
    "            props = []\n",
    "            for p in self.property_list:\n",
    "                props.append(row[p])\n",
    "            properties.append(props)\n",
    "\n",
    "        self.smiles_encodings = torch.tensor(smiles_encoding)\n",
    "        self.properties = torch.tensor(properties)\n",
    "        # self.affinities = torch.tensor(affinities)\n",
    "        print(\"dataset built\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.properties)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"smiles_rep\": self.smiles_encodings[index],\n",
    "            \"properties\": self.properties[index],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset built\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomTargetDataset(train_df, smiles_vocab, ['affinity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"Attention is All You Need positional encoding layer\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model, p_dropout,n=10000):\n",
    "        \"\"\"Initializes the layer.\"\"\"\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
    "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
    "        angles = token_positions / (n ** ((2 * dim_positions) / d_model))\n",
    "\n",
    "        encodings = torch.zeros(1, seq_len, d_model)\n",
    "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
    "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
    "        encodings.requires_grad = False\n",
    "        self.register_buffer(\"positional_encodings\", encodings)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs forward pass of the module.\"\"\"\n",
    "        x = x + self.positional_encodings[:,:x.shape[1],:]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_properties):\n",
    "        super(PropertyEncoder, self).__init__()\n",
    "        self.layer = nn.Linear(n_properties, d_model)\n",
    "        self.layer_final = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.layer_final(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmileDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, vocab, n_properties, hidden_units=1024, dropout=0.1):\n",
    "        super(SmileDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab = vocab\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(len(vocab), d_model)\n",
    "        self.smile_pe = PositionalEncodings(SMI_MAX_SIZE, d_model, dropout)\n",
    "        \n",
    "        self.trfmLayer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                    nhead=n_heads,\n",
    "                                                    dim_feedforward=hidden_units,\n",
    "                                                    dropout=dropout,\n",
    "                                                    batch_first=True,\n",
    "                                                    norm_first=True,\n",
    "                                                    activation=\"gelu\")\n",
    "        self.trfm = nn.TransformerEncoder(encoder_layer=self.trfmLayer,\n",
    "                                          num_layers=n_layers,\n",
    "                                          norm=nn.LayerNorm(d_model))\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.final = nn.Linear(d_model, 1)\n",
    "        self.property_encoder = PropertyEncoder(d_model,n_properties=n_properties)\n",
    "        \n",
    "    def forward(self, x, property):\n",
    "        # property = self.property_encoder(property).unsqueeze(1)\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x = self.smile_pe(x)\n",
    "        \n",
    "        # x = torch.cat([property, x], dim=1).to(x.device)\n",
    "        \n",
    "        # mask = set_up_causal_mask(x.shape[1]).to(device)\n",
    "        x = self.trfm(src=x,\n",
    "                      mask=None,\n",
    "                      )\n",
    "        x = self.ln_f(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SmileDecoder(d_model=512, n_heads=8, n_layers=6, vocab=smiles_vocab, n_properties=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "out = net(data['smiles_rep'].to(device), data['properties'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbdd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
